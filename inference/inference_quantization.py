"""
é‡åŒ–æ¨ç†æ¨¡å— - InferenceOnQuantization

ä½¿ç”¨transformersåº“å®ç°æ¨¡å‹é‡åŒ–çš„å¯¹æ¯”å®éªŒï¼Œå¯¹æ¯”ç»„æ˜¯æ ‡å‡†ç²¾åº¦æ¨ç†ï¼Œå®éªŒç»„æ˜¯é‡åŒ–æ¨ç†ã€‚
è¿™æ„æˆäº†é’ˆå¯¹é‡åŒ–ä¼˜åŒ–æŠ€æœ¯çš„å¯¹æ¯”å®éªŒã€‚
"""

from typing import List, Optional, Any, Dict
import gc
import warnings
warnings.filterwarnings("ignore")

from config.config import Config
from inference.inference_module import InferenceModule


class InferenceOnQuantization(InferenceModule):
    """åŸºäºé‡åŒ–çš„æ¨ç†æ¨¡å—"""
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–é‡åŒ–æ¨ç†æ¨¡å—
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        super().__init__(config)
        
        # å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹
        self.standard_model = None
        self.standard_tokenizer = None
        self.quantized_model = None
        self.quantized_tokenizer = None
        
        print(f"åˆå§‹åŒ–é‡åŒ–æ¨ç†æ¨¡å—")
        print(f"æ¨¡å‹: {config.model}")
        print(f"è®¾å¤‡: {config.get_device_str()}")
        print(f"é‡åŒ–é…ç½®: {config.quantization_config}")
    
    def _initialize_standard_model(self):
        """åˆå§‹åŒ–æ ‡å‡†ç²¾åº¦æ¨¡å‹"""
        if self.standard_model is not None:
            return
        
        print("æ­£åœ¨åŠ è½½æ ‡å‡†ç²¾åº¦æ¨¡å‹...")
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
            
            # åŠ è½½tokenizer
            self.standard_tokenizer = AutoTokenizer.from_pretrained(self.config.model)
            if self.standard_tokenizer.pad_token is None:
                self.standard_tokenizer.pad_token = self.standard_tokenizer.eos_token
            
            # åŠ è½½æ ‡å‡†ç²¾åº¦æ¨¡å‹
            device_map = "auto" if self.config.is_multi_gpu() else self.config.get_device_str()
            
            self.standard_model = AutoModelForCausalLM.from_pretrained(
                self.config.model,
                device_map=device_map,
                torch_dtype=torch.float16 if "cuda" in self.config.get_device_str() else torch.float32,
                trust_remote_code=True
            )
            
            print(f"æ ‡å‡†ç²¾åº¦æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device_map}")
            
        except Exception as e:
            print(f"åŠ è½½æ ‡å‡†ç²¾åº¦æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _initialize_quantized_model(self):
        """åˆå§‹åŒ–é‡åŒ–æ¨¡å‹"""
        if self.quantized_model is not None:
            return
        
        print("æ­£åœ¨åŠ è½½é‡åŒ–æ¨¡å‹...")
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
            import torch
            
            # åŠ è½½tokenizer
            self.quantized_tokenizer = AutoTokenizer.from_pretrained(self.config.model)
            if self.quantized_tokenizer.pad_token is None:
                self.quantized_tokenizer.pad_token = self.quantized_tokenizer.eos_token
            
            # é…ç½®é‡åŒ–å‚æ•°
            quantization_config = None
            if self.config.quantization_config:
                quant_config = self.config.quantization_config
                
                if quant_config.get('load_in_4bit', False):
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=getattr(torch, quant_config.get('bnb_4bit_compute_dtype', 'float16')),
                        bnb_4bit_use_double_quant=quant_config.get('bnb_4bit_use_double_quant', True),
                        bnb_4bit_quant_type=quant_config.get('bnb_4bit_quant_type', 'nf4')
                    )
                elif quant_config.get('load_in_8bit', False):
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True
                    )
            
            # åŠ è½½é‡åŒ–æ¨¡å‹
            device_map = "auto" if self.config.is_multi_gpu() else self.config.get_device_str()
            
            model_kwargs = {
                "device_map": device_map,
                "trust_remote_code": True
            }
            
            if quantization_config is not None:
                model_kwargs["quantization_config"] = quantization_config
            else:
                # å¦‚æœæ²¡æœ‰é‡åŒ–é…ç½®ï¼Œä½¿ç”¨æ ‡å‡†float16
                model_kwargs["torch_dtype"] = torch.float16 if "cuda" in self.config.get_device_str() else torch.float32
            
            self.quantized_model = AutoModelForCausalLM.from_pretrained(
                self.config.model,
                **model_kwargs
            )
            
            print(f"é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device_map}")
            if quantization_config:
                print(f"é‡åŒ–é…ç½®: {quantization_config}")
            
        except Exception as e:
            print(f"åŠ è½½é‡åŒ–æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def basic_inference(self, prompts: List[str]) -> List[str]:
        """
        åŸºäºæ ‡å‡†ç²¾åº¦çš„åŸºç¡€æ¨ç†ï¼ˆå¯¹ç…§ç»„ï¼‰
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        self._initialize_standard_model()
        
        try:
            import torch
            
            outputs = []
            
            for prompt in prompts:
                # è®°å½•é¦–tokenç”Ÿæˆæ—¶é—´
                with self.performance_monitor.measure_first_token():
                    # ç¼–ç è¾“å…¥
                    inputs = self.standard_tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        padding=True, 
                        truncation=True,
                        max_length=512
                    )
                    
                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    device = self.config.get_device_str()
                    if device != "cpu":
                        inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆæ–‡æœ¬
                    with torch.no_grad():
                        generated_ids = self.standard_model.generate(
                            **inputs,
                            max_new_tokens=self.config.max_tokens,
                            temperature=self.config.temperature,
                            top_p=self.config.top_p,
                            do_sample=True,
                            pad_token_id=self.standard_tokenizer.eos_token_id,
                            eos_token_id=self.standard_tokenizer.eos_token_id,
                        )
                
                # è§£ç è¾“å‡º
                generated_text = self.standard_tokenizer.decode(
                    generated_ids[0], 
                    skip_special_tokens=True
                )
                
                # ç§»é™¤åŸå§‹promptï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
                generated_only = generated_text[len(prompt):].strip()
                outputs.append(generated_only)
            
            return outputs
            
        except Exception as e:
            print(f"æ ‡å‡†ç²¾åº¦æ¨ç†å¤±è´¥: {e}")
            return [f"Error: {e}"] * len(prompts)
    
    def optimized_inference(self, prompts: List[str]) -> List[str]:
        """
        åŸºäºé‡åŒ–çš„ä¼˜åŒ–æ¨ç†ï¼ˆå®éªŒç»„ï¼‰
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        self._initialize_quantized_model()
        
        try:
            import torch
            
            outputs = []
            
            for prompt in prompts:
                # è®°å½•é¦–tokenç”Ÿæˆæ—¶é—´
                with self.performance_monitor.measure_first_token():
                    # ç¼–ç è¾“å…¥
                    inputs = self.quantized_tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        padding=True, 
                        truncation=True,
                        max_length=512
                    )
                    
                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    device = self.config.get_device_str()
                    if device != "cpu":
                        inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆæ–‡æœ¬
                    with torch.no_grad():
                        generated_ids = self.quantized_model.generate(
                            **inputs,
                            max_new_tokens=self.config.max_tokens,
                            temperature=self.config.temperature,
                            top_p=self.config.top_p,
                            do_sample=True,
                            pad_token_id=self.quantized_tokenizer.eos_token_id,
                            eos_token_id=self.quantized_tokenizer.eos_token_id,
                        )
                
                # è§£ç è¾“å‡º
                generated_text = self.quantized_tokenizer.decode(
                    generated_ids[0], 
                    skip_special_tokens=True
                )
                
                # ç§»é™¤åŸå§‹promptï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
                generated_only = generated_text[len(prompt):].strip()
                outputs.append(generated_only)
            
            return outputs
            
        except Exception as e:
            print(f"é‡åŒ–æ¨ç†å¤±è´¥: {e}")
            return [f"Error: {e}"] * len(prompts)
    
    def inference(self, prompts: List[str], method: str = "both") -> Dict[str, Any]:
        """
        é‡å†™æ¨ç†æ–¹æ³•ä»¥æ”¯æŒè®¾å¤‡é—´çš„å†…å­˜ç®¡ç†
        """
        results = {}
        
        if method in ["basic", "both"]:
            print(f"\nå¼€å§‹åŸºç¡€æ¨ç†æµ‹è¯•...")
            basic_results, basic_metrics = self._run_inference_with_monitoring(
                prompts, self.basic_inference, "åŸºç¡€æ¨ç†"
            )
            results["basic"] = {
                "outputs": basic_results,
                "metrics": basic_metrics
            }
            
            # å¦‚æœè¦è¿è¡Œä¸¤ç§æ–¹æ³•ï¼Œåœ¨åˆ‡æ¢åˆ°ä¼˜åŒ–æ¨ç†å‰æ¸…ç†åŸºç¡€æ¨ç†æ¨¡å‹
            if method == "both":
                print("æ¸…ç†åŸºç¡€æ¨ç†æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜...")
                if self.standard_model is not None:
                    del self.standard_model
                    self.standard_model = None
                if self.standard_tokenizer is not None:
                    del self.standard_tokenizer
                    self.standard_tokenizer = None
                
                gc.collect()
                
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        print("CUDAç¼“å­˜å·²æ¸…ç†")
                except ImportError:
                    pass
        
        if method in ["optimized", "both"]:
            print(f"\nå¼€å§‹ä¼˜åŒ–æ¨ç†æµ‹è¯•...")
            optimized_results, optimized_metrics = self._run_inference_with_monitoring(
                prompts, self.optimized_inference, "ä¼˜åŒ–æ¨ç†"
            )
            results["optimized"] = {
                "outputs": optimized_results,
                "metrics": optimized_metrics
            }
        
        # å¦‚æœä¸¤ç§æ–¹æ³•éƒ½è¿è¡Œäº†ï¼Œè¿›è¡Œå¯¹æ¯”
        if method == "both":
            print(f"\nå¼€å§‹æ€§èƒ½å¯¹æ¯”...")
            self.compare_results(results["basic"]["metrics"], results["optimized"]["metrics"])
        
        return results
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("æ­£åœ¨æ¸…ç†èµ„æº...")
        
        # æ¸…ç†æ ‡å‡†æ¨¡å‹
        if self.standard_model is not None:
            del self.standard_model
            self.standard_model = None
        
        if self.standard_tokenizer is not None:
            del self.standard_tokenizer
            self.standard_tokenizer = None
        
        # æ¸…ç†é‡åŒ–æ¨¡å‹
        if self.quantized_model is not None:
            del self.quantized_model
            self.quantized_model = None
        
        if self.quantized_tokenizer is not None:
            del self.quantized_tokenizer
            self.quantized_tokenizer = None
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # æ¸…ç†CUDAç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print("CUDAç¼“å­˜å·²æ¸…ç†")
        except ImportError:
            pass
        
        print("èµ„æºæ¸…ç†å®Œæˆ")
    
    def run_comparison_test(self, custom_prompts: Optional[List[str]] = None) -> dict:
        """
        è¿è¡Œå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•
        
        Args:
            custom_prompts: è‡ªå®šä¹‰æç¤ºåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æç¤º
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print("\n" + "="*80)
        print("é‡åŒ– vs æ ‡å‡†ç²¾åº¦ æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("="*80)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        if custom_prompts is None:
            prompts = self.generate_test_prompts()
        else:
            prompts = custom_prompts
        
        print(f"æµ‹è¯•é…ç½®:")
        print(f"  æ¨¡å‹: {self.config.model}")
        print(f"  è®¾å¤‡: {self.config.get_device_str()}")
        print(f"  æç¤ºæ•°é‡: {len(prompts)}")
        print(f"  æœ€å¤§ç”Ÿæˆtokens: {self.config.max_tokens}")
        print(f"  æµ‹è¯•è¿­ä»£æ¬¡æ•°: {self.config.test_iterations}")
        print(f"  é¢„çƒ­è¿­ä»£æ¬¡æ•°: {self.config.warmup_iterations}")
        print(f"  é‡åŒ–é…ç½®: {self.config.quantization_config}")
        
        try:
            # è¿è¡Œå¯¹æ¯”æµ‹è¯•
            results = self.inference(prompts, method="both")
            
            # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            self._generate_summary_report(results, prompts)
            
            return results
            
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}
        finally:
            # ç¡®ä¿èµ„æºè¢«æ¸…ç†
            self.cleanup()
    
    def _generate_summary_report(self, results: dict, prompts: List[str]):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        if "basic" not in results or "optimized" not in results:
            return
        
        basic_metrics = results["basic"]["metrics"]
        optimized_metrics = results["optimized"]["metrics"]
        
        print(f"\n" + "="*80)
        print("æµ‹è¯•æ€»ç»“")
        print("="*80)
        
        print(f"æµ‹è¯•æ¦‚è¿°:")
        print(f"  âœ“ åŸºç¡€æ¨ç† (æ ‡å‡†ç²¾åº¦): å¹³å‡å»¶è¿Ÿ {basic_metrics.avg_latency*1000:.2f}ms")
        print(f"  âœ“ ä¼˜åŒ–æ¨ç† (é‡åŒ–): å¹³å‡å»¶è¿Ÿ {optimized_metrics.avg_latency*1000:.2f}ms")
        
        # è®¡ç®—æ”¹è¿›
        latency_improvement = (basic_metrics.avg_latency - optimized_metrics.avg_latency) / basic_metrics.avg_latency * 100
        throughput_improvement = (optimized_metrics.tokens_per_second - basic_metrics.tokens_per_second) / basic_metrics.tokens_per_second * 100
        memory_improvement = (basic_metrics.gpu_memory_peak - optimized_metrics.gpu_memory_peak) / basic_metrics.gpu_memory_peak * 100
        
        print(f"\né‡åŒ–ä¼˜åŒ–æ•ˆæœ:")
        if latency_improvement > 0:
            print(f"  ğŸš€ å»¶è¿Ÿé™ä½: {latency_improvement:.1f}%")
        if throughput_improvement > 0:
            print(f"  ğŸ“ˆ ååé‡æå‡: {throughput_improvement:.1f}%")
        if memory_improvement > 0:
            print(f"  ğŸ’¾ å†…å­˜èŠ‚çœ: {memory_improvement:.1f}%")
        
        print(f"\nç»“è®º:")
        if memory_improvement > 20:
            print(f"  âœ… é‡åŒ–æ˜¾è‘—èŠ‚çœäº†å†…å­˜ä½¿ç”¨")
        elif memory_improvement > 0:
            print(f"  âœ… é‡åŒ–é€‚åº¦èŠ‚çœäº†å†…å­˜ä½¿ç”¨")
        else:
            print(f"  âš ï¸  åœ¨å½“å‰æµ‹è¯•åœºæ™¯ä¸‹ï¼Œå†…å­˜èŠ‚çœæ•ˆæœä¸æ˜æ˜¾")
        
        if throughput_improvement > 10:
            print(f"  âœ… é‡åŒ–æ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½")
        elif throughput_improvement > 0:
            print(f"  âœ… é‡åŒ–é€‚åº¦æå‡äº†æ¨ç†æ€§èƒ½")
        else:
            print(f"  âš ï¸  é‡åŒ–å¯èƒ½åœ¨é€Ÿåº¦ä¸Šæœ‰è½»å¾®æŸå¤±ï¼Œä½†èŠ‚çœäº†å†…å­˜")
        
        print("="*80)
    
    def benchmark_memory_efficiency(self, batch_sizes: Optional[List[int]] = None) -> dict:
        """
        æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°ä¸‹çš„å†…å­˜æ•ˆç‡
        
        Args:
            batch_sizes: è¦æµ‹è¯•çš„æ‰¹æ¬¡å¤§å°åˆ—è¡¨
            
        Returns:
            å†…å­˜æ•ˆç‡æµ‹è¯•ç»“æœ
        """
        if batch_sizes is None:
            batch_sizes = [1, 2, 4, 8]
        
        print(f"\nå†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•")
        print(f"æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_sizes}")
        
        results = {}
        base_prompts = self.generate_test_prompts()
        
        for batch_size in batch_sizes:
            if batch_size > len(base_prompts):
                prompts = base_prompts * ((batch_size // len(base_prompts)) + 1)
                prompts = prompts[:batch_size]
            else:
                prompts = base_prompts[:batch_size]
            
            print(f"\næµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            original_iterations = self.config.test_iterations
            self.config.test_iterations = 2 
            
            try:
                batch_results = self.inference(prompts, method="both")
                results[batch_size] = batch_results
                
            except Exception as e:
                print(f"æ‰¹æ¬¡å¤§å° {batch_size} æµ‹è¯•å¤±è´¥: {e}")
                results[batch_size] = {"error": str(e)}
            finally:
                self.config.test_iterations = original_iterations
                self.cleanup()
        
        return results
