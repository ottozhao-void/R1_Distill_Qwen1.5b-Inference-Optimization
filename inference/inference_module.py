"""
æ¨ç†æ¨¡å—åŸºç±» - InferenceModule

è¯¥æ¨¡å—å®šä¹‰äº†æ¨ç†æ¨¡å—çš„åŸºç¡€æ¥å£ï¼Œæ‰€æœ‰å…·ä½“çš„æ¨ç†æ¨¡å—éƒ½åº”è¯¥ç»§æ‰¿è¿™ä¸ªåŸºç±»ã€‚
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from utils.performance_monitor import PerformanceMonitor, PerformanceMetrics
from config.config import Config


class InferenceModule(ABC):
    """æ¨ç†æ¨¡å—åŸºç±»"""
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–æ¨ç†æ¨¡å—
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        # ä»configä¸­è·å–è®¾å¤‡IDç”¨äºGPUå†…å­˜ç›‘æµ‹
        device_id = None
        if config.device and len(config.device) > 0 and config.device[0] != 'cpu':
            device_id = config.device[0]
        self.performance_monitor = PerformanceMonitor(device_id=device_id)
        self.is_initialized = False
    
    @abstractmethod
    def basic_inference(self, prompts: List[str]) -> List[str]:
        """
        åŸºç¡€æ¨ç†æ–¹æ³•ï¼ˆå¯¹ç…§ç»„ï¼‰
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        pass
    
    @abstractmethod
    def optimized_inference(self, prompts: List[str]) -> List[str]:
        """
        ä¼˜åŒ–æ¨ç†æ–¹æ³•ï¼ˆå®éªŒç»„ï¼‰
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        pass
    
    def inference(self, prompts: List[str], method: str = "both") -> Dict[str, Any]:
        """
        æ¨¡å‹æ¨ç†å…¥å£ï¼Œè°ƒç”¨åŸºç¡€æ¨ç†å’Œä¼˜åŒ–æ¨ç†ï¼Œå¹¶è¿›è¡Œæ€§èƒ½ç›‘æµ‹
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            method: æ¨ç†æ–¹æ³• ("basic", "optimized", "both")
            
        Returns:
            åŒ…å«ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        results = {}
        
        if method in ["basic", "both"]:
            print(f"\nå¼€å§‹åŸºç¡€æ¨ç†æµ‹è¯•...")
            basic_results, basic_metrics = self._run_inference_with_monitoring(
                prompts, self.basic_inference, "åŸºç¡€æ¨ç†"
            )
            results["basic"] = {
                "outputs": basic_results,
                "metrics": basic_metrics
            }
        
        if method in ["optimized", "both"]:
            print(f"\nå¼€å§‹ä¼˜åŒ–æ¨ç†æµ‹è¯•...")
            optimized_results, optimized_metrics = self._run_inference_with_monitoring(
                prompts, self.optimized_inference, "ä¼˜åŒ–æ¨ç†"
            )
            results["optimized"] = {
                "outputs": optimized_results,
                "metrics": optimized_metrics
            }
        
        # å¦‚æœä¸¤ç§æ–¹æ³•éƒ½è¿è¡Œäº†ï¼Œè¿›è¡Œå¯¹æ¯”
        if method == "both":
            print(f"\nå¼€å§‹æ€§èƒ½å¯¹æ¯”...")
            self.compare_results(results["basic"]["metrics"], results["optimized"]["metrics"])
        
        return results
    
    def _run_inference_with_monitoring(self, prompts: List[str], 
                                     inference_func, method_name: str) -> tuple:
        """
        è¿è¡Œæ¨ç†å¹¶ç›‘æµ‹æ€§èƒ½
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            inference_func: æ¨ç†å‡½æ•°
            method_name: æ–¹æ³•åç§°
            
        Returns:
            (outputs, metrics) å…ƒç»„
        """
        import torch
        
        # é‡ç½®ç›‘æµ‹å™¨ - æ¸…ç†ä¹‹å‰çš„å†…å­˜ç»Ÿè®¡
        self.performance_monitor.reset()
        
        # è®°å½•å¼€å§‹å‰çš„å†…å­˜çŠ¶æ€
        initial_gpu_info = self.performance_monitor.get_gpu_memory_info()
        print(f"å¼€å§‹å‰GPUå†…å­˜: å·²åˆ†é…={initial_gpu_info['allocated']:.2f}MB, å³°å€¼={initial_gpu_info['peak']:.2f}MB, ç³»ç»Ÿä½¿ç”¨={initial_gpu_info['system_used']:.2f}MB")
        
        # é¢„çƒ­é˜¶æ®µ
        print(f"é¢„çƒ­é˜¶æ®µ...")
        warmup_prompts = prompts[:min(len(prompts), self.config.warmup_iterations)]
        for _ in range(self.config.warmup_iterations):
            _ = inference_func(warmup_prompts)
        
        # é¢„çƒ­åè®°å½•å†…å­˜å¹¶é‡ç½®ç›‘æµ‹
        warmup_gpu_info = self.performance_monitor.get_gpu_memory_info()
        print(f"é¢„çƒ­åGPUå†…å­˜: å·²åˆ†é…={warmup_gpu_info['allocated']:.2f}MB, å³°å€¼={warmup_gpu_info['peak']:.2f}MB, ç³»ç»Ÿä½¿ç”¨={warmup_gpu_info['system_used']:.2f}MB")
        
        # é‡ç½®ç³»ç»Ÿå†…å­˜åŸºçº¿ï¼ˆä»¥é¢„çƒ­åçš„çŠ¶æ€ä¸ºåŸºå‡†ï¼‰
        self.performance_monitor.system_memory_tracker.reset_baseline()
        self.performance_monitor.start_monitoring()
        
        # æ­£å¼æµ‹è¯•
        print(f"æ­£å¼æµ‹è¯•é˜¶æ®µ...")
        all_outputs = []
        
        for i in range(self.config.test_iterations):
            with self.performance_monitor.measure_batch():
                outputs = inference_func(prompts)
                all_outputs.extend(outputs)
                
                # è®°å½•tokenæ•°é‡
                total_tokens = sum(len(output.split()) for output in outputs)
                self.performance_monitor.add_token_count(total_tokens)
        
        self.performance_monitor.stop_monitoring()
        
        # è®°å½•ç»“æŸåçš„å†…å­˜çŠ¶æ€
        final_gpu_info = self.performance_monitor.get_gpu_memory_info()
        system_memory_increase = self.performance_monitor.system_memory_tracker.get_memory_increase()
        print(f"ç»“æŸåGPUå†…å­˜: å·²åˆ†é…={final_gpu_info['allocated']:.2f}MB, å³°å€¼={final_gpu_info['peak']:.2f}MB, ç³»ç»Ÿä½¿ç”¨={final_gpu_info['system_used']:.2f}MB, ç³»ç»Ÿå¢é‡={system_memory_increase:.2f}MB")
        
        # è·å–æ€§èƒ½æŒ‡æ ‡
        metrics = self.performance_monitor.get_metrics(
            model_name=self.config.model,
            batch_size=len(prompts)
        )
        
        # æ‰“å°ç»“æœ
        self.print_result(metrics, method_name)
        
        return all_outputs, metrics
    
    def print_result(self, metrics: PerformanceMetrics, method_name: str):
        """
        æ‰“å°æ€§èƒ½ç›‘æµ‹ç»“æœå¹¶ä¿å­˜æŠ¥å‘Š
        
        Args:
            metrics: æ€§èƒ½æŒ‡æ ‡
            method_name: æ–¹æ³•åç§°
        """
        print(f"\n{method_name} æ€§èƒ½ç»“æœ:")
        self.performance_monitor.print_summary(metrics)
        
        # ä¿å­˜æŠ¥å‘Š
        if self.config.save_results:
            import os
            
            # ä¿å­˜JSONæ ¼å¼è¯¦ç»†æŠ¥å‘Š
            json_filename = f"{method_name.replace(' ', '_').lower()}_report.json"
            json_filepath = os.path.join(self.config.output_dir, json_filename)
            self.performance_monitor.save_detailed_report(metrics, json_filepath)
            
            # ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š
            md_filename = f"{method_name.replace(' ', '_').lower()}_report.md"
            md_filepath = os.path.join(self.config.output_dir, md_filename)
            
            # åˆ›å»ºé…ç½®å­—å…¸
            config_dict = {
                'experiment_name': getattr(self.config, 'experiment_name', 'default'),
                'technique': getattr(self.config, 'technique', 'Unknown'),
                'batch_level': getattr(self.config, 'batch_level', 'medium'),
                'model': self.config.model,
                'device': self.config.device,
                'batch_size': self.config.batch_size,
                'max_tokens': self.config.max_tokens,
                'temperature': self.config.temperature,
                'top_p': self.config.top_p,
                'num_prompts': self.config.num_prompts,
                'warmup_iterations': self.config.warmup_iterations,
                'test_iterations': self.config.test_iterations,
                'quantization_config': getattr(self.config, 'quantization_config', None)
            }
            
            self.performance_monitor.save_markdown_report(
                metrics, config_dict, md_filepath, method_name
            )
    
    def compare_results(self, basic_metrics: PerformanceMetrics, 
                       optimized_metrics: PerformanceMetrics):
        """
        æ¯”è¾ƒåŸºç¡€æ¨ç†å’Œä¼˜åŒ–æ¨ç†çš„ç»“æœ
        
        Args:
            basic_metrics: åŸºç¡€æ¨ç†æ€§èƒ½æŒ‡æ ‡
            optimized_metrics: ä¼˜åŒ–æ¨ç†æ€§èƒ½æŒ‡æ ‡
        """
        from utils.performance_monitor import compare_metrics
        compare_metrics(basic_metrics, optimized_metrics, "åŸºç¡€æ¨ç†", "ä¼˜åŒ–æ¨ç†")
        
        # ä¿å­˜æ¯”è¾ƒæŠ¥å‘Š
        if self.config.save_results:
            self._save_comparison_report(basic_metrics, optimized_metrics)
    
    def _save_comparison_report(self, basic_metrics: PerformanceMetrics, 
                              optimized_metrics: PerformanceMetrics):
        """ä¿å­˜å¯¹æ¯”å®éªŒçš„ç»¼åˆæŠ¥å‘Š"""
        import os
        from datetime import datetime
        
        # åˆ›å»ºé…ç½®å­—å…¸
        config_dict = {
            'experiment_name': getattr(self.config, 'experiment_name', 'default'),
            'technique': getattr(self.config, 'technique', 'Unknown'),
            'batch_level': getattr(self.config, 'batch_level', 'medium'),
            'model': self.config.model,
            'device': self.config.device,
            'batch_size': self.config.batch_size,
            'max_tokens': self.config.max_tokens,
            'temperature': self.config.temperature,
            'top_p': self.config.top_p,
            'num_prompts': self.config.num_prompts,
            'warmup_iterations': self.config.warmup_iterations,
            'test_iterations': self.config.test_iterations,
            'quantization_config': getattr(self.config, 'quantization_config', None)
        }
        
        # è®¡ç®—æ”¹è¿›æŒ‡æ ‡
        latency_improvement = 0
        throughput_improvement = 0 
        memory_improvement = 0
        ttft_improvement = 0
        tpot_improvement = 0
        
        if basic_metrics.avg_latency > 0:
            latency_improvement = (basic_metrics.avg_latency - optimized_metrics.avg_latency) / basic_metrics.avg_latency * 100
        
        if basic_metrics.tokens_per_second > 0:
            throughput_improvement = (optimized_metrics.tokens_per_second - basic_metrics.tokens_per_second) / basic_metrics.tokens_per_second * 100
        
        if basic_metrics.gpu_memory_peak > 0:
            memory_improvement = (basic_metrics.gpu_memory_peak - optimized_metrics.gpu_memory_peak) / basic_metrics.gpu_memory_peak * 100
        
        if basic_metrics.time_to_first_token > 0:
            ttft_improvement = (basic_metrics.time_to_first_token - optimized_metrics.time_to_first_token) / basic_metrics.time_to_first_token * 100
        
        if basic_metrics.time_per_output_token > 0:
            tpot_improvement = (basic_metrics.time_per_output_token - optimized_metrics.time_per_output_token) / basic_metrics.time_per_output_token * 100
        
        report_md = f"""# {config_dict['technique']} å¯¹æ¯”å®éªŒå®Œæ•´æŠ¥å‘Š

## å®éªŒæ¦‚è¿°
**å®éªŒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**å®éªŒåç§°**: {config_dict['experiment_name']}  
**ä¼˜åŒ–æŠ€æœ¯**: {config_dict['technique']}  
**æ‰¹æ¬¡çº§åˆ«**: {config_dict['batch_level']}  

## å®éªŒé…ç½®

### æ¨¡å‹é…ç½®
- **æ¨¡å‹**: {config_dict['model']}
- **è®¾å¤‡**: {config_dict['device']}
- **æ‰¹æ¬¡å¤§å°**: {config_dict['batch_size']}

### æ¨ç†å‚æ•°
- **æœ€å¤§tokens**: {config_dict['max_tokens']}
- **æ¸©åº¦**: {config_dict['temperature']}
- **Top-p**: {config_dict['top_p']}

### æµ‹è¯•å‚æ•°
- **æç¤ºæ•°é‡**: {config_dict['num_prompts']}
- **é¢„çƒ­è¿­ä»£**: {config_dict['warmup_iterations']}
- **æµ‹è¯•è¿­ä»£**: {config_dict['test_iterations']}

"""
        
        if config_dict.get('quantization_config'):
            import json
            report_md += f"""### é‡åŒ–é…ç½®
```yaml
{json.dumps(config_dict['quantization_config'], indent=2)}
```

"""
        
        report_md += f"""## åŸºç¡€æ¨ç†å®éªŒç»“æœ

### å»¶è¿ŸæŒ‡æ ‡
- **æ€»æ—¶é—´**: {basic_metrics.total_time:.3f}s
- **å¹³å‡å»¶è¿Ÿ**: {basic_metrics.avg_latency*1000:.2f}ms
- **P50å»¶è¿Ÿ**: {basic_metrics.p50_latency*1000:.2f}ms
- **P95å»¶è¿Ÿ**: {basic_metrics.p95_latency*1000:.2f}ms
- **P99å»¶è¿Ÿ**: {basic_metrics.p99_latency*1000:.2f}ms
- **é¦–tokenæ—¶é—´**: {basic_metrics.time_to_first_token*1000:.2f}ms
- **æ¯è¾“å‡ºtokenæ—¶é—´**: {basic_metrics.time_per_output_token*1000:.2f}ms

### ååé‡æŒ‡æ ‡
- **Tokens/ç§’**: {basic_metrics.tokens_per_second:.2f}
- **è¯·æ±‚/ç§’**: {basic_metrics.requests_per_second:.2f}

### å†…å­˜ä½¿ç”¨
- **GPUå†…å­˜å³°å€¼**: {basic_metrics.gpu_memory_peak:.2f}MB
- **GPUå†…å­˜å½“å‰**: {basic_metrics.gpu_memory_used:.2f}MB
- **CPUå†…å­˜**: {basic_metrics.cpu_memory_used:.2f}MB

## ä¼˜åŒ–æ¨ç†å®éªŒç»“æœ

### å»¶è¿ŸæŒ‡æ ‡
- **æ€»æ—¶é—´**: {optimized_metrics.total_time:.3f}s
- **å¹³å‡å»¶è¿Ÿ**: {optimized_metrics.avg_latency*1000:.2f}ms
- **P50å»¶è¿Ÿ**: {optimized_metrics.p50_latency*1000:.2f}ms
- **P95å»¶è¿Ÿ**: {optimized_metrics.p95_latency*1000:.2f}ms
- **P99å»¶è¿Ÿ**: {optimized_metrics.p99_latency*1000:.2f}ms
- **é¦–tokenæ—¶é—´**: {optimized_metrics.time_to_first_token*1000:.2f}ms
- **æ¯è¾“å‡ºtokenæ—¶é—´**: {optimized_metrics.time_per_output_token*1000:.2f}ms

### ååé‡æŒ‡æ ‡
- **Tokens/ç§’**: {optimized_metrics.tokens_per_second:.2f}
- **è¯·æ±‚/ç§’**: {optimized_metrics.requests_per_second:.2f}

### å†…å­˜ä½¿ç”¨
- **GPUå†…å­˜å³°å€¼**: {optimized_metrics.gpu_memory_peak:.2f}MB
- **GPUå†…å­˜å½“å‰**: {optimized_metrics.gpu_memory_used:.2f}MB
- **CPUå†…å­˜**: {optimized_metrics.cpu_memory_used:.2f}MB

## æ€§èƒ½æ”¹è¿›åˆ†æ

### æ”¹è¿›æŒ‡æ ‡å¯¹æ¯”
| æŒ‡æ ‡ | åŸºç¡€æ¨ç† | ä¼˜åŒ–æ¨ç† | æ”¹è¿› |
|------|----------|----------|------|
| å¹³å‡å»¶è¿Ÿ | {basic_metrics.avg_latency*1000:.2f}ms | {optimized_metrics.avg_latency*1000:.2f}ms | {latency_improvement:+.1f}% |
| ååé‡ | {basic_metrics.tokens_per_second:.2f} tokens/s | {optimized_metrics.tokens_per_second:.2f} tokens/s | {throughput_improvement:+.1f}% |
| GPUå†…å­˜å³°å€¼ | {basic_metrics.gpu_memory_peak:.2f}MB | {optimized_metrics.gpu_memory_peak:.2f}MB | {memory_improvement:+.1f}% |
| é¦–tokenæ—¶é—´ | {basic_metrics.time_to_first_token*1000:.2f}ms | {optimized_metrics.time_to_first_token*1000:.2f}ms | {ttft_improvement:+.1f}% |
| æ¯è¾“å‡ºtokenæ—¶é—´ | {basic_metrics.time_per_output_token*1000:.2f}ms | {optimized_metrics.time_per_output_token*1000:.2f}ms | {tpot_improvement:+.1f}% |

### ä¼˜åŒ–æ•ˆæœæ€»ç»“
"""
        
        if throughput_improvement > 10:
            report_md += f"- âœ… **æ˜¾è‘—æ€§èƒ½æå‡**: {config_dict['technique']}æŠ€æœ¯æ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½\n"
        elif throughput_improvement > 0:
            report_md += f"- âœ… **é€‚åº¦æ€§èƒ½æå‡**: {config_dict['technique']}æŠ€æœ¯é€‚åº¦æå‡äº†æ¨ç†æ€§èƒ½\n"
        else:
            report_md += f"- âš ï¸ **æ€§èƒ½æ— æ˜æ˜¾æå‡**: åœ¨å½“å‰æµ‹è¯•åœºæ™¯ä¸‹ï¼Œ{config_dict['technique']}ä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾\n"
        
        if memory_improvement > 10:
            report_md += f"- ğŸ’¾ **æ˜¾è‘—å†…å­˜èŠ‚çœ**: å†…å­˜ä½¿ç”¨å‡å°‘äº†{memory_improvement:.1f}%\n"
        elif memory_improvement > 0:
            report_md += f"- ğŸ’¾ **é€‚åº¦å†…å­˜èŠ‚çœ**: å†…å­˜ä½¿ç”¨å‡å°‘äº†{memory_improvement:.1f}%\n"
        
        if latency_improvement > 10:
            report_md += f"- ğŸš€ **æ˜¾è‘—å»¶è¿Ÿé™ä½**: å¹³å‡å»¶è¿Ÿé™ä½äº†{latency_improvement:.1f}%\n"
        elif latency_improvement > 0:
            report_md += f"- ğŸš€ **é€‚åº¦å»¶è¿Ÿé™ä½**: å¹³å‡å»¶è¿Ÿé™ä½äº†{latency_improvement:.1f}%\n"
        
        report_md += f"""
## ç»“è®º
æœ¬æ¬¡å®éªŒå¯¹æ¯”äº†åŸºç¡€æ¨ç†ä¸ä½¿ç”¨{config_dict['technique']}æŠ€æœ¯çš„ä¼˜åŒ–æ¨ç†æ€§èƒ½ã€‚"""
        
        if throughput_improvement > 0 and memory_improvement > 0:
            report_md += f"ç»“æœæ˜¾ç¤º{config_dict['technique']}æŠ€æœ¯åœ¨æå‡æ¨ç†é€Ÿåº¦å’ŒèŠ‚çœå†…å­˜æ–¹é¢éƒ½æœ‰è‰¯å¥½è¡¨ç°ã€‚"
        elif throughput_improvement > 0:
            report_md += f"ç»“æœæ˜¾ç¤º{config_dict['technique']}æŠ€æœ¯ä¸»è¦åœ¨æå‡æ¨ç†é€Ÿåº¦æ–¹é¢æœ‰è‰¯å¥½è¡¨ç°ã€‚"
        elif memory_improvement > 0:
            report_md += f"ç»“æœæ˜¾ç¤º{config_dict['technique']}æŠ€æœ¯ä¸»è¦åœ¨èŠ‚çœå†…å­˜ä½¿ç”¨æ–¹é¢æœ‰è‰¯å¥½è¡¨ç°ã€‚"
        else:
            report_md += f"åœ¨å½“å‰æµ‹è¯•é…ç½®ä¸‹ï¼Œ{config_dict['technique']}æŠ€æœ¯çš„ä¼˜åŒ–æ•ˆæœæœ‰é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é…ç½®å‚æ•°æˆ–åœ¨æ›´å¤§è§„æ¨¡çš„æµ‹è¯•ä¸­éªŒè¯æ•ˆæœã€‚"
        
        report_md += f"""

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜å®Œæ•´å¯¹æ¯”æŠ¥å‘Š
        comparison_filename = f"comparison_experiment_{config_dict['batch_level']}_{config_dict['experiment_name']}.md"
        comparison_filepath = os.path.join(self.config.output_dir, comparison_filename)
        
        try:
            with open(comparison_filepath, 'w', encoding='utf-8') as f:
                f.write(report_md)
            print(f"å®Œæ•´å¯¹æ¯”å®éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {comparison_filepath}")
        except Exception as e:
            print(f"ä¿å­˜å¯¹æ¯”å®éªŒæŠ¥å‘Šå¤±è´¥: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return {
            "model_name": self.config.model,
            "device": self.config.get_device_str(),
            "tensor_parallel_size": self.config.get_tensor_parallel_size(),
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature,
            "top_p": self.config.top_p
        }
    
    def generate_test_prompts(self) -> List[str]:
        """
        ç”Ÿæˆæµ‹è¯•ç”¨çš„æç¤ºåˆ—è¡¨
        
        Returns:
            æµ‹è¯•æç¤ºåˆ—è¡¨
        """
        base_prompts = [
            "The future of artificial intelligence is",
            "In a world where technology advances rapidly,",
            "The most important discovery in science was",
            "Climate change affects our planet by",
            "The key to successful leadership involves",
            "Machine learning algorithms can help us",
            "The evolution of human civilization shows",
            "Renewable energy sources are important because",
            "The impact of social media on society",
            "Space exploration has taught us that",
            "The benefits of education include",
            "Sustainable development requires",
            "The role of government in modern society",
            "Innovation drives progress through",
            "The importance of healthcare systems"
        ]
        
        # æ ¹æ®é…ç½®ä¸­çš„num_promptsé€‰æ‹©æç¤º
        selected_prompts = base_prompts[:min(len(base_prompts), self.config.num_prompts)]
        return selected_prompts
    
    @abstractmethod
    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        pass
