"""
é…ç½®æ¨¡å— - Config

è¯¥æ¨¡å—ç”¨æ¥ç®¡ç†é¡¹ç›®ä¸­çš„å„é¡¹é…ç½®ï¼ŒåŒ…æ‹¬æ¨¡å‹è·¯å¾„ã€è®¾å¤‡é…ç½®ç­‰ã€‚
æ”¯æŒä»YAMLæ–‡ä»¶åŠ è½½å®éªŒé…ç½®ï¼Œæ¯ä¸ªä¼˜åŒ–æŠ€æœ¯æœ‰ç‹¬ç«‹çš„é…ç½®æ–‡ä»¶ã€‚

æ³¨æ„ï¼šæœ¬é¡¹ç›®å·²é…ç½®ä¸ºç»Ÿä¸€ä½¿ç”¨ DeepSeek-R1-Distill-Qwen-1.5B æ¨¡å‹ï¼Œ
æ‰€æœ‰é…ç½®éƒ½å·²æ›´æ–°ä¸ºä½¿ç”¨æ­¤æ¨¡å‹ï¼Œç¡®ä¿æ•´ä¸ªæ¨ç†å·¥ä½œæµçš„ä¸€è‡´æ€§ã€‚
"""

import os
import yaml
from typing import List, Optional, Dict, Any
from dataclasses import dataclass, field


@dataclass
class Config:
    """é¡¹ç›®é…ç½®ç±»"""
    
    # å®éªŒé…ç½®
    experiment_name: str = "default"
    technique: str = "PagedAttention"  # PagedAttention, Quantization
    batch_level: str = "medium"  # small, medium, large
    
    # æ¨¡å‹é…ç½®
    model: str = "/data1/zhaofanghan/.cache/huggingface/hub/DeepSeek-R1-Distill-Qwen-1.5B"
    device: Optional[List[int]] = field(default_factory=lambda: [4])  # CUDAè®¾å¤‡IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
    
    # æ¨ç†é…ç½®
    max_tokens: int = 100
    temperature: float = 0.8
    top_p: float = 0.95
    batch_size: int = 4
    
    # æ€§èƒ½æµ‹è¯•é…ç½®
    num_prompts: int = 10
    warmup_iterations: int = 3
    test_iterations: int = 5
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "results"
    save_results: bool = True
    
    # é‡åŒ–ç›¸å…³é…ç½®
    quantization_config: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        """åˆå§‹åŒ–åçš„å¤„ç†"""
        if self.device is None:
            # è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„CUDAè®¾å¤‡
            import torch
            if torch.cuda.is_available():
                self.device = list(range(torch.cuda.device_count()))
            else:
                self.device = []
        
        # æ ¹æ®batch_levelè°ƒæ•´batch_size
        if self.batch_level == "small":
            self.batch_size = min(self.batch_size, 2)
            self.num_prompts = min(self.num_prompts, 5)
        elif self.batch_level == "large":
            self.batch_size = max(self.batch_size, 8)
            self.num_prompts = max(self.num_prompts, 20)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆæ ¹æ®æŠ€æœ¯ç±»å‹ã€æ‰¹æ¬¡çº§åˆ«å’Œæ—¶é—´æˆ³ï¼‰
        if self.save_results:
            technique_dir = os.path.join("results", self.technique)
            if not os.path.exists(technique_dir):
                os.makedirs(technique_dir)
            
            # åªæœ‰åœ¨output_dirè¿˜æ˜¯é»˜è®¤å€¼æ—¶æ‰æ›´æ–°
            if self.output_dir == "results":
                # ç”Ÿæˆæ—¶é—´æˆ³å­—ç¬¦ä¸²
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # åˆ›å»ºå¸¦æ‰¹æ¬¡çº§åˆ«å’Œæ—¶é—´æˆ³çš„ç›®å½•
                batch_timestamp_dir = os.path.join(technique_dir, f"{self.batch_level}_{timestamp}")
                if not os.path.exists(batch_timestamp_dir):
                    os.makedirs(batch_timestamp_dir)
                
                self.output_dir = batch_timestamp_dir
    
    def get_tensor_parallel_size(self) -> int:
        """è·å–å¼ é‡å¹¶è¡Œå¤§å°"""
        return len(self.device) if self.device else 1
    
    def get_device_str(self) -> str:
        """è·å–è®¾å¤‡å­—ç¬¦ä¸²"""
        if not self.device:
            return "cpu"
        elif len(self.device) == 1:
            return f"cuda:{self.device[0]}"
        else:
            return f"cuda:{self.device[0]}"  # ä¸»è®¾å¤‡
    
    def is_multi_gpu(self) -> bool:
        """æ˜¯å¦ä½¿ç”¨å¤šGPU"""
        return len(self.device) > 1 if self.device else False


def load_config_from_yaml(config_path: str) -> Config:
    """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = yaml.safe_load(f)
        
        # åˆ›å»ºConfigå¯¹è±¡
        config = Config()
        
        # æ›´æ–°é…ç½®å­—æ®µ
        for key, value in config_data.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # æ‰‹åŠ¨è°ƒç”¨__post_init__è¿›è¡Œåå¤„ç†
        config.__post_init__()
        
        return config
        
    except Exception as e:
        print(f"ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®å¤±è´¥: {e}")
        print("ä½¿ç”¨é»˜è®¤é…ç½®")
        return Config()


def save_config_to_yaml(config: Config, config_path: str):
    """ä¿å­˜é…ç½®åˆ°YAMLæ–‡ä»¶"""
    try:
        # åˆ›å»ºç›®å½•
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        
        # è½¬æ¢ä¸ºå­—å…¸
        config_dict = {
            'experiment_name': config.experiment_name,
            'technique': config.technique,
            'batch_level': config.batch_level,
            'model': config.model,
            'device': config.device,
            'max_tokens': config.max_tokens,
            'temperature': config.temperature,
            'top_p': config.top_p,
            'batch_size': config.batch_size,
            'num_prompts': config.num_prompts,
            'warmup_iterations': config.warmup_iterations,
            'test_iterations': config.test_iterations,
            'output_dir': config.output_dir,
            'save_results': config.save_results,
            'quantization_config': config.quantization_config
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
        
        print(f"é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
        
    except Exception as e:
        print(f"ä¿å­˜é…ç½®åˆ°YAMLæ–‡ä»¶å¤±è´¥: {e}")


def get_config_for_technique(technique: str, batch_level: str = "medium", 
                           experiment_name: str = "default") -> Config:
    """æ ¹æ®ä¼˜åŒ–æŠ€æœ¯è·å–é…ç½®"""
    config_dir = "config"
    config_filename = f"{technique.lower()}_{batch_level}_{experiment_name}.yaml"
    config_path = os.path.join(config_dir, technique, config_filename)
    
    if os.path.exists(config_path):
        return load_config_from_yaml(config_path)
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config = Config()
        config.technique = technique
        config.batch_level = batch_level
        config.experiment_name = experiment_name
        
        # ä¿å­˜é»˜è®¤é…ç½®
        save_config_to_yaml(config, config_path)
        return config


def get_available_devices() -> Dict[str, Any]:
    """è·å–å¯ç”¨è®¾å¤‡ä¿¡æ¯"""
    device_info = {"has_cuda": False, "cuda_devices": [], "cpu_available": True}
    
    try:
        import torch
        if torch.cuda.is_available():
            device_info["has_cuda"] = True
            device_count = torch.cuda.device_count()
            for i in range(device_count):
                device_name = torch.cuda.get_device_name(i)
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
                device_info["cuda_devices"].append({
                    "id": i,
                    "name": device_name,
                    "memory_gb": round(memory_total, 1)
                })
    except ImportError:
        pass
    
    return device_info


def print_available_devices():
    """æ‰“å°å¯ç”¨è®¾å¤‡ä¿¡æ¯"""
    device_info = get_available_devices()
    
    print("ğŸ–¥ï¸  å¯ç”¨è®¾å¤‡ä¿¡æ¯:")
    print("="*50)
    
    if device_info["has_cuda"]:
        print("âœ… CUDA è®¾å¤‡:")
        for device in device_info["cuda_devices"]:
            print(f"   GPU {device['id']}: {device['name']} ({device['memory_gb']}GB)")
        print(f"\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print(f"   --device 0          # ä½¿ç”¨ GPU 0")
        if len(device_info["cuda_devices"]) > 1:
            print(f"   --device 0,1        # ä½¿ç”¨ GPU 0 å’Œ 1")
    else:
        print("âŒ æœªæ£€æµ‹åˆ° CUDA è®¾å¤‡")
    
    if device_info["cpu_available"]:
        print(f"âœ… CPU å¯ç”¨")
        print(f"   --device cpu        # ä½¿ç”¨ CPU")
    
    print("="*50)


def load_config_from_env() -> Config:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
    config = Config()
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    model_name = os.getenv("MODEL_NAME")
    if model_name:
        config.model = model_name
    
    cuda_devices = os.getenv("CUDA_DEVICES")
    if cuda_devices:
        device_ids = [int(x) for x in cuda_devices.split(",")]
        config.device = device_ids
    
    max_tokens = os.getenv("MAX_TOKENS")
    if max_tokens:
        config.max_tokens = int(max_tokens)
    
    temperature = os.getenv("TEMPERATURE")
    if temperature:
        config.temperature = float(temperature)
    
    output_dir = os.getenv("OUTPUT_DIR")
    if output_dir:
        config.output_dir = output_dir
    
    return config
