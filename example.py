"""
ç¤ºä¾‹è„šæœ¬ - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å„ä¸ªæ¨¡å—

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨é¡¹ç›®çš„å„ä¸ªæ¨¡å—ï¼Œ
è€Œä¸æ˜¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ã€‚é€‚åˆé›†æˆåˆ°å…¶ä»–é¡¹ç›®ä¸­ã€‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import Config, ConfigPresets
from inference_paged_attention import InferenceOnPagedAttention
from performance_monitor import compare_metrics


def example_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸ”§ åŸºç¡€ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºé…ç½®
    config = ConfigPresets.small_model_config()
    config.test_iterations = 2  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´
    config.warmup_iterations = 1
    
    print(f"ä½¿ç”¨æ¨¡å‹: {config.model}")
    
    # åˆ›å»ºæ¨ç†æ¨¡å—
    inference_module = InferenceOnPagedAttention(config)
    
    try:
        # å‡†å¤‡æµ‹è¯•æç¤º
        prompts = [
            "The future of AI is",
            "Machine learning will help us",
            "Climate change affects"
        ]
        
        print(f"æµ‹è¯•æç¤ºæ•°é‡: {len(prompts)}")
        
        # è¿è¡Œæ¨ç†æµ‹è¯•
        results = inference_module.inference(prompts, method="both")
        
        # æ‰“å°ä¸€äº›ç”Ÿæˆçš„ç¤ºä¾‹
        if "basic" in results and "optimized" in results:
            print("\nğŸ“ ç”Ÿæˆç¤ºä¾‹:")
            basic_outputs = results["basic"]["outputs"]
            optimized_outputs = results["optimized"]["outputs"]
            
            for i, prompt in enumerate(prompts[:2]):  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ª
                if i < len(basic_outputs) and i < len(optimized_outputs):
                    print(f"\næç¤º: {prompt}")
                    print(f"åŸºç¡€æ¨ç†: {basic_outputs[i][:100]}...")
                    print(f"ä¼˜åŒ–æ¨ç†: {optimized_outputs[i][:100]}...")
        
        return results
        
    except Exception as e:
        print(f"âŒ åŸºç¡€ä½¿ç”¨ç¤ºä¾‹å¤±è´¥: {e}")
        return None
    finally:
        inference_module.cleanup()


def example_custom_config():
    """è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹"""
    print("\nğŸ”§ è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    config = Config(
        model="facebook/opt-125m",  # ä½¿ç”¨å°æ¨¡å‹
        max_tokens=30,
        temperature=0.9,
        num_prompts=3,
        test_iterations=2,
        warmup_iterations=1,
        output_dir="custom_results"
    )
    
    print(f"è‡ªå®šä¹‰é…ç½®:")
    print(f"  æ¨¡å‹: {config.model}")
    print(f"  æœ€å¤§tokens: {config.max_tokens}")
    print(f"  æ¸©åº¦: {config.temperature}")
    
    inference_module = InferenceOnPagedAttention(config)
    
    try:
        # ä½¿ç”¨è‡ªå®šä¹‰æç¤º
        custom_prompts = [
            "Tell me about",
            "The best way to",
            "In my opinion"
        ]
        
        results = inference_module.inference(custom_prompts, method="optimized")
        
        if "optimized" in results:
            print("\nâœ… è‡ªå®šä¹‰é…ç½®æµ‹è¯•å®Œæˆ")
            metrics = results["optimized"]["metrics"]
            print(f"å¹³å‡å»¶è¿Ÿ: {metrics.avg_latency*1000:.2f}ms")
            print(f"ååé‡: {metrics.tokens_per_second:.2f} tokens/s")
        
        return results
        
    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹å¤±è´¥: {e}")
        return None
    finally:
        inference_module.cleanup()


def example_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹"""
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹")
    print("=" * 50)
    
    config = ConfigPresets.small_model_config()
    config.test_iterations = 2
    
    inference_module = InferenceOnPagedAttention(config)
    
    try:
        prompts = ["Artificial intelligence is", "The future holds"]
        
        # åˆ†åˆ«è¿è¡Œä¸¤ç§æ–¹æ³•
        print("è¿è¡ŒåŸºç¡€æ¨ç†...")
        basic_results = inference_module.inference(prompts, method="basic")
        
        print("è¿è¡Œä¼˜åŒ–æ¨ç†...")
        optimized_results = inference_module.inference(prompts, method="optimized")
        
        # æ‰‹åŠ¨å¯¹æ¯”ç»“æœ
        if "basic" in basic_results and "optimized" in optimized_results:
            basic_metrics = basic_results["basic"]["metrics"]
            optimized_metrics = optimized_results["optimized"]["metrics"]
            
            print("\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
            print(f"åŸºç¡€æ¨ç†å»¶è¿Ÿ: {basic_metrics.avg_latency*1000:.2f}ms")
            print(f"ä¼˜åŒ–æ¨ç†å»¶è¿Ÿ: {optimized_metrics.avg_latency*1000:.2f}ms")
            
            improvement = (basic_metrics.avg_latency - optimized_metrics.avg_latency) / basic_metrics.avg_latency * 100
            print(f"å»¶è¿Ÿæ”¹å–„: {improvement:.1f}%")
            
            # ä½¿ç”¨å†…ç½®çš„å¯¹æ¯”å‡½æ•°
            compare_metrics(basic_metrics, optimized_metrics, "ä¼ ç»Ÿæ–¹æ³•", "PagedAttention")
        
        return basic_results, optimized_results
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹å¤±è´¥: {e}")
        return None, None
    finally:
        inference_module.cleanup()


def example_memory_benchmark():
    """å†…å­˜åŸºå‡†æµ‹è¯•ç¤ºä¾‹"""
    print("\nğŸ’¾ å†…å­˜åŸºå‡†æµ‹è¯•ç¤ºä¾‹")
    print("=" * 50)
    
    config = ConfigPresets.small_model_config()
    config.test_iterations = 1  # å‡å°‘æ—¶é—´
    
    inference_module = InferenceOnPagedAttention(config)
    
    try:
        # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
        batch_sizes = [1, 2]
        results = inference_module.benchmark_memory_efficiency(batch_sizes)
        
        print("\nğŸ“Š å†…å­˜ä½¿ç”¨å¯¹æ¯”:")
        for batch_size, result in results.items():
            if "error" not in result:
                basic_memory = result["basic"]["metrics"].gpu_memory_peak
                optimized_memory = result["optimized"]["metrics"].gpu_memory_peak
                print(f"æ‰¹æ¬¡å¤§å° {batch_size}:")
                print(f"  åŸºç¡€æ–¹æ³•: {basic_memory:.1f}MB")
                print(f"  ä¼˜åŒ–æ–¹æ³•: {optimized_memory:.1f}MB")
                if basic_memory > 0:
                    savings = (basic_memory - optimized_memory) / basic_memory * 100
                    print(f"  å†…å­˜èŠ‚çœ: {savings:.1f}%")
        
        return results
        
    except Exception as e:
        print(f"âŒ å†…å­˜åŸºå‡†æµ‹è¯•ç¤ºä¾‹å¤±è´¥: {e}")
        return None
    finally:
        inference_module.cleanup()


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸ¯ LLMæ¨ç†ä¼˜åŒ–é¡¹ç›® - ç¤ºä¾‹è„šæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥åŸºæœ¬ä¾èµ–
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…ï¼ŒæŸäº›ç¤ºä¾‹å¯èƒ½æ— æ³•è¿è¡Œ")
    
    try:
        import transformers
        print(f"âœ… Transformersç‰ˆæœ¬: {transformers.__version__}")
    except ImportError:
        print("âŒ Transformersæœªå®‰è£…")
        return
    
    try:
        import vllm
        print(f"âœ… vLLMå¯ç”¨")
    except ImportError:
        print("âŒ vLLMæœªå®‰è£…")
        return
    
    # è¿è¡Œç¤ºä¾‹
    try:
        # ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨
        result1 = example_basic_usage()
        
        # ç¤ºä¾‹2: è‡ªå®šä¹‰é…ç½®
        result2 = example_custom_config()
        
        # ç¤ºä¾‹3: æ€§èƒ½å¯¹æ¯”
        result3a, result3b = example_performance_comparison()
        
        # ç¤ºä¾‹4: å†…å­˜åŸºå‡†æµ‹è¯•
        result4 = example_memory_benchmark()
        
        print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ’¡ æ›´å¤šç”¨æ³•è¯·å‚è€ƒ:")
        print("  - main.py: å‘½ä»¤è¡Œæ¥å£")
        print("  - README.md: è¯¦ç»†æ–‡æ¡£")
        print("  - run_benchmark.sh: è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç¤ºä¾‹è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
