"""
PagedAttentionæ¨ç†æ¨¡å— - InferenceOnPagedAttention

ä½¿ç”¨vLLMåº“å®ç°åŸºäºPagedAttentionçš„æ¨¡å‹æ¨ç†ï¼Œå¯¹æ¯”ç»„æ˜¯åŸºäºtransformersåº“çš„ç›¸åŒæ¨¡å‹çš„æ¨ç†ã€‚
è¿™æ„æˆäº†é’ˆå¯¹PagedAttentionä¼˜åŒ–æŠ€æœ¯çš„å¯¹æ¯”å®éªŒã€‚
"""

from typing import List, Optional, Any, Dict
import gc
import warnings
warnings.filterwarnings("ignore")

from config import Config
from inference_module import InferenceModule


class InferenceOnPagedAttention(InferenceModule):
    """åŸºäºPagedAttentionçš„æ¨ç†æ¨¡å—"""
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–PagedAttentionæ¨ç†æ¨¡å—
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        super().__init__(config)
        
        # å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹
        self.transformers_model = None
        self.transformers_tokenizer = None
        self.vllm_model = None
        
        print(f"åˆå§‹åŒ–PagedAttentionæ¨ç†æ¨¡å—")
        print(f"æ¨¡å‹: {config.model}")
        print(f"è®¾å¤‡: {config.get_device_str()}")
        print(f"æ˜¯å¦å¤šGPU: {config.is_multi_gpu()}")
    
    def _initialize_transformers_model(self):
        """åˆå§‹åŒ–transformersæ¨¡å‹"""
        if self.transformers_model is not None:
            return
        
        print("æ­£åœ¨åŠ è½½transformersæ¨¡å‹...")
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
            
            # åŠ è½½tokenizer
            self.transformers_tokenizer = AutoTokenizer.from_pretrained(self.config.model)
            if self.transformers_tokenizer.pad_token is None:
                self.transformers_tokenizer.pad_token = self.transformers_tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            device_map = "auto" if self.config.is_multi_gpu() else self.config.get_device_str()
            
            self.transformers_model = AutoModelForCausalLM.from_pretrained(
                self.config.model,
                device_map=device_map,
                torch_dtype=torch.float16 if "cuda" in self.config.get_device_str() else torch.float32,
                trust_remote_code=True
            )
            
            print(f"transformersæ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device_map}")
            
        except Exception as e:
            print(f"åŠ è½½transformersæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _initialize_vllm_model(self):
        """åˆå§‹åŒ–vLLMæ¨¡å‹"""
        if self.vllm_model is not None:
            return
        
        print("æ­£åœ¨åŠ è½½vLLMæ¨¡å‹...")
        try:
            from vllm import LLM
            
            # vLLMåˆå§‹åŒ–å‚æ•°
            vllm_kwargs = {
                "model": self.config.model,
                "trust_remote_code": True,
                "max_model_len": 2048,  # é™åˆ¶åºåˆ—é•¿åº¦ä»¥èŠ‚çœå†…å­˜
                "enforce_eager": True,  # ç¦ç”¨torchç¼–è¯‘ä»¥é¿å…ldconfigé—®é¢˜
            }
            
            # è®¾ç½®GPUç›¸å…³å‚æ•°
            if self.config.device and len(self.config.device) > 0:
                vllm_kwargs["tensor_parallel_size"] = self.config.get_tensor_parallel_size()
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªGPUæˆ–CPUï¼Œè®¾ç½®ç›¸åº”å‚æ•°
            if self.config.device and len(self.config.device) == 1:
                vllm_kwargs["gpu_memory_utilization"] = 0.5  # Reduced to avoid conflicts
            
            self.vllm_model = LLM(**vllm_kwargs)
            
            print(f"vLLMæ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼ é‡å¹¶è¡Œåº¦: {self.config.get_tensor_parallel_size()}")
            
        except Exception as e:
            print(f"åŠ è½½vLLMæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def basic_inference(self, prompts: List[str]) -> List[str]:
        """
        åŸºäºtransformersåº“çš„åŸºç¡€æ¨ç†ï¼ˆå¯¹ç…§ç»„ï¼‰
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        self._initialize_transformers_model()
        
        try:
            import torch
            
            outputs = []
            
            for prompt in prompts:
                # ç¼–ç è¾“å…¥
                inputs = self.transformers_tokenizer(
                    prompt, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=512
                )
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                device = self.config.get_device_str()
                if device != "cpu":
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                
                # ç”Ÿæˆæ–‡æœ¬
                with torch.no_grad():
                    generated_ids = self.transformers_model.generate(
                        **inputs,
                        max_new_tokens=self.config.max_tokens,
                        temperature=self.config.temperature,
                        top_p=self.config.top_p,
                        do_sample=True,
                        pad_token_id=self.transformers_tokenizer.eos_token_id,
                        eos_token_id=self.transformers_tokenizer.eos_token_id,
                    )
                
                # è§£ç è¾“å‡º
                generated_text = self.transformers_tokenizer.decode(
                    generated_ids[0], 
                    skip_special_tokens=True
                )
                
                # ç§»é™¤åŸå§‹promptï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
                generated_only = generated_text[len(prompt):].strip()
                outputs.append(generated_only)
            
            return outputs
            
        except Exception as e:
            print(f"transformersæ¨ç†å¤±è´¥: {e}")
            return [f"Error: {e}"] * len(prompts)
    
    def optimized_inference(self, prompts: List[str]) -> List[str]:
        """
        åŸºäºvLLMåº“çš„ä¼˜åŒ–æ¨ç†ï¼ˆå®éªŒç»„ï¼‰
        ä½¿ç”¨PagedAttentionä¼˜åŒ–å†…å­˜ç®¡ç†
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        self._initialize_vllm_model()
        
        try:
            from vllm import SamplingParams
            
            # è®¾ç½®é‡‡æ ·å‚æ•°
            sampling_params = SamplingParams(
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                max_tokens=self.config.max_tokens,
            )
            
            # ä½¿ç”¨vLLMè¿›è¡Œæ‰¹é‡æ¨ç†
            outputs = self.vllm_model.generate(prompts, sampling_params)
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_texts = []
            for output in outputs:
                generated_text = output.outputs[0].text.strip()
                generated_texts.append(generated_text)
            
            return generated_texts
            
        except Exception as e:
            print(f"vLLMæ¨ç†å¤±è´¥: {e}")
            return [f"Error: {e}"] * len(prompts)
    
    def inference(self, prompts: List[str], method: str = "both") -> Dict[str, Any]:
        """
        é‡å†™æ¨ç†æ–¹æ³•ä»¥æ”¯æŒè®¾å¤‡é—´çš„å†…å­˜ç®¡ç†
        """
        results = {}
        
        if method in ["basic", "both"]:
            print(f"\nå¼€å§‹åŸºç¡€æ¨ç†æµ‹è¯•...")
            basic_results, basic_metrics = self._run_inference_with_monitoring(
                prompts, self.basic_inference, "åŸºç¡€æ¨ç†"
            )
            results["basic"] = {
                "outputs": basic_results,
                "metrics": basic_metrics
            }
            
            # å¦‚æœè¦è¿è¡Œä¸¤ç§æ–¹æ³•ï¼Œåœ¨åˆ‡æ¢åˆ°ä¼˜åŒ–æ¨ç†å‰æ¸…ç†åŸºç¡€æ¨ç†æ¨¡å‹
            if method == "both":
                print("æ¸…ç†åŸºç¡€æ¨ç†æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜...")
                if self.transformers_model is not None:
                    del self.transformers_model
                    self.transformers_model = None
                if self.transformers_tokenizer is not None:
                    del self.transformers_tokenizer
                    self.transformers_tokenizer = None
                
                import gc
                gc.collect()
                
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        print("CUDAç¼“å­˜å·²æ¸…ç†")
                except ImportError:
                    pass
        
        if method in ["optimized", "both"]:
            print(f"\nå¼€å§‹ä¼˜åŒ–æ¨ç†æµ‹è¯•...")
            optimized_results, optimized_metrics = self._run_inference_with_monitoring(
                prompts, self.optimized_inference, "ä¼˜åŒ–æ¨ç†"
            )
            results["optimized"] = {
                "outputs": optimized_results,
                "metrics": optimized_metrics
            }
        
        # å¦‚æœä¸¤ç§æ–¹æ³•éƒ½è¿è¡Œäº†ï¼Œè¿›è¡Œå¯¹æ¯”
        if method == "both":
            print(f"\nå¼€å§‹æ€§èƒ½å¯¹æ¯”...")
            self.compare_results(results["basic"]["metrics"], results["optimized"]["metrics"])
        
        return results
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("æ­£åœ¨æ¸…ç†èµ„æº...")
        
        # æ¸…ç†transformersæ¨¡å‹
        if self.transformers_model is not None:
            del self.transformers_model
            self.transformers_model = None
        
        if self.transformers_tokenizer is not None:
            del self.transformers_tokenizer
            self.transformers_tokenizer = None
        
        # æ¸…ç†vLLMæ¨¡å‹
        if self.vllm_model is not None:
            del self.vllm_model
            self.vllm_model = None
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # æ¸…ç†CUDAç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print("CUDAç¼“å­˜å·²æ¸…ç†")
        except ImportError:
            pass
        
        print("èµ„æºæ¸…ç†å®Œæˆ")
    
    def run_comparison_test(self, custom_prompts: Optional[List[str]] = None) -> dict:
        """
        è¿è¡Œå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•
        
        Args:
            custom_prompts: è‡ªå®šä¹‰æç¤ºåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æç¤º
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print("\n" + "="*80)
        print("PagedAttention vs Traditional Key-Value Cache æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("="*80)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        if custom_prompts is None:
            prompts = self.generate_test_prompts()
        else:
            prompts = custom_prompts
        
        print(f"æµ‹è¯•é…ç½®:")
        print(f"  æ¨¡å‹: {self.config.model}")
        print(f"  è®¾å¤‡: {self.config.get_device_str()}")
        print(f"  æç¤ºæ•°é‡: {len(prompts)}")
        print(f"  æœ€å¤§ç”Ÿæˆtokens: {self.config.max_tokens}")
        print(f"  æµ‹è¯•è¿­ä»£æ¬¡æ•°: {self.config.test_iterations}")
        print(f"  é¢„çƒ­è¿­ä»£æ¬¡æ•°: {self.config.warmup_iterations}")
        
        try:
            # è¿è¡Œå¯¹æ¯”æµ‹è¯•
            results = self.inference(prompts, method="both")
            
            # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            self._generate_summary_report(results, prompts)
            
            return results
            
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}
        finally:
            # ç¡®ä¿èµ„æºè¢«æ¸…ç†
            self.cleanup()
    
    def _generate_summary_report(self, results: dict, prompts: List[str]):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        if "basic" not in results or "optimized" not in results:
            return
        
        basic_metrics = results["basic"]["metrics"]
        optimized_metrics = results["optimized"]["metrics"]
        
        print(f"\n" + "="*80)
        print("æµ‹è¯•æ€»ç»“")
        print("="*80)
        
        print(f"æµ‹è¯•æ¦‚è¿°:")
        print(f"  âœ“ åŸºç¡€æ¨ç† (transformers): å¹³å‡å»¶è¿Ÿ {basic_metrics.avg_latency*1000:.2f}ms")
        print(f"  âœ“ ä¼˜åŒ–æ¨ç† (vLLM+PagedAttention): å¹³å‡å»¶è¿Ÿ {optimized_metrics.avg_latency*1000:.2f}ms")
        
        # è®¡ç®—æ”¹è¿›
        latency_improvement = (basic_metrics.avg_latency - optimized_metrics.avg_latency) / basic_metrics.avg_latency * 100
        throughput_improvement = (optimized_metrics.tokens_per_second - basic_metrics.tokens_per_second) / basic_metrics.tokens_per_second * 100
        memory_improvement = (basic_metrics.gpu_memory_peak - optimized_metrics.gpu_memory_peak) / basic_metrics.gpu_memory_peak * 100
        
        print(f"\nPagedAttentionä¼˜åŒ–æ•ˆæœ:")
        if latency_improvement > 0:
            print(f"  ğŸš€ å»¶è¿Ÿé™ä½: {latency_improvement:.1f}%")
        if throughput_improvement > 0:
            print(f"  ğŸ“ˆ ååé‡æå‡: {throughput_improvement:.1f}%")
        if memory_improvement > 0:
            print(f"  ğŸ’¾ å†…å­˜èŠ‚çœ: {memory_improvement:.1f}%")
        
        print(f"\nç»“è®º:")
        if throughput_improvement > 10:
            print(f"  âœ… PagedAttentionæ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½")
        elif throughput_improvement > 0:
            print(f"  âœ… PagedAttentioné€‚åº¦æå‡äº†æ¨ç†æ€§èƒ½")
        else:
            print(f"  âš ï¸  åœ¨å½“å‰æµ‹è¯•åœºæ™¯ä¸‹ï¼Œä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾")
        
        print("="*80)
    
    def benchmark_memory_efficiency(self, batch_sizes: Optional[List[int]] = None) -> dict:
        """
        æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°ä¸‹çš„å†…å­˜æ•ˆç‡
        
        Args:
            batch_sizes: è¦æµ‹è¯•çš„æ‰¹æ¬¡å¤§å°åˆ—è¡¨
            
        Returns:
            å†…å­˜æ•ˆç‡æµ‹è¯•ç»“æœ
        """
        if batch_sizes is None:
            batch_sizes = [1, 2, 4, 8]
        
        print(f"\nå†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•")
        print(f"æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_sizes}")
        
        results = {}
        base_prompts = self.generate_test_prompts()
        
        for batch_size in batch_sizes:
            if batch_size > len(base_prompts):
                prompts = base_prompts * ((batch_size // len(base_prompts)) + 1)
                prompts = prompts[:batch_size]
            else:
                prompts = base_prompts[:batch_size]
            
            print(f"\næµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            # ä¸´æ—¶ä¿®æ”¹é…ç½®
            original_iterations = self.config.test_iterations
            self.config.test_iterations = 2  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´
            
            try:
                batch_results = self.inference(prompts, method="both")
                results[batch_size] = batch_results
                
            except Exception as e:
                print(f"æ‰¹æ¬¡å¤§å° {batch_size} æµ‹è¯•å¤±è´¥: {e}")
                results[batch_size] = {"error": str(e)}
            finally:
                self.config.test_iterations = original_iterations
                self.cleanup()
        
        return results
