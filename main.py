"""
ä¸»ç¨‹åº - å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’Œä¼˜åŒ–

è¿™æ˜¯é¡¹ç›®çš„ä¸»å…¥å£æ–‡ä»¶ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å„ä¸ªæ¨¡å—è¿›è¡ŒLLMæ¨ç†æ€§èƒ½å¯¹æ¯”æµ‹è¯•ã€‚
"""

import argparse
import sys
import os
from typing import List, Optional

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import Config, ConfigPresets, load_config_from_env
from inference_paged_attention import InferenceOnPagedAttention


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="LLMæ¨ç†ä¼˜åŒ–æ€§èƒ½æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤å°æ¨¡å‹è¿›è¡Œå¿«é€Ÿæµ‹è¯•
  python main.py --preset small
  
  # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
  python main.py --model facebook/opt-1.3b --max-tokens 200
  
  # åªè¿è¡ŒåŸºç¡€æ¨ç†
  python main.py --method basic
  
  # åªè¿è¡Œä¼˜åŒ–æ¨ç†
  python main.py --method optimized
  
  # ä½¿ç”¨è‡ªå®šä¹‰GPUè®¾å¤‡
  python main.py --devices 0,1 --model meta-llama/Llama-2-7b-chat-hf
  
  # å†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•
  python main.py --benchmark-memory --batch-sizes 1,2,4,8
        """
    )
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model", type=str, default=None,
                       help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--devices", type=str, default=None,
                       help="CUDAè®¾å¤‡IDï¼Œç”¨é€—å·åˆ†éš” (ä¾‹å¦‚: 0,1)")
    
    # æ¨ç†é…ç½®
    parser.add_argument("--max-tokens", type=int, default=None,
                       help="æœ€å¤§ç”Ÿæˆtokensæ•°é‡")
    parser.add_argument("--temperature", type=float, default=None,
                       help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=None,
                       help="æ ¸é‡‡æ ·å‚æ•°")
    parser.add_argument("--batch-size", type=int, default=None,
                       help="æ‰¹æ¬¡å¤§å°")
    
    # æµ‹è¯•é…ç½®
    parser.add_argument("--num-prompts", type=int, default=None,
                       help="æµ‹è¯•æç¤ºæ•°é‡")
    parser.add_argument("--test-iterations", type=int, default=None,
                       help="æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--warmup-iterations", type=int, default=None,
                       help="é¢„çƒ­è¿­ä»£æ¬¡æ•°")
    
    # é¢„è®¾é…ç½®
    parser.add_argument("--preset", type=str, choices=["small", "large", "llama"],
                       help="ä½¿ç”¨é¢„è®¾é…ç½®")
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--method", type=str, 
                       choices=["basic", "optimized", "both"], 
                       default="both",
                       help="æ¨ç†æ–¹æ³•")
    
    # åŸºå‡†æµ‹è¯•
    parser.add_argument("--benchmark-memory", action="store_true",
                       help="è¿è¡Œå†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•")
    parser.add_argument("--batch-sizes", type=str, default="1,2,4,8",
                       help="å†…å­˜åŸºå‡†æµ‹è¯•çš„æ‰¹æ¬¡å¤§å°ï¼Œç”¨é€—å·åˆ†éš”")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--output-dir", type=str, default=None,
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--no-save", action="store_true",
                       help="ä¸ä¿å­˜ç»“æœæ–‡ä»¶")
    
    # è‡ªå®šä¹‰æç¤º
    parser.add_argument("--prompts", nargs="+", default=None,
                       help="è‡ªå®šä¹‰æµ‹è¯•æç¤º")
    parser.add_argument("--prompts-file", type=str, default=None,
                       help="ä»æ–‡ä»¶è¯»å–æµ‹è¯•æç¤º")
    
    return parser.parse_args()


def load_prompts_from_file(filepath: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æç¤º"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f if line.strip()]
        print(f"ä»æ–‡ä»¶ {filepath} åŠ è½½äº† {len(prompts)} ä¸ªæç¤º")
        return prompts
    except Exception as e:
        print(f"æ— æ³•ä»æ–‡ä»¶åŠ è½½æç¤º: {e}")
        return []


def create_config_from_args(args) -> Config:
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®"""
    
    # ä½¿ç”¨é¢„è®¾é…ç½®
    if args.preset:
        if args.preset == "small":
            config = ConfigPresets.small_model_config()
        elif args.preset == "large":
            config = ConfigPresets.large_model_config()
        elif args.preset == "llama":
            config = ConfigPresets.llama_config()
    else:
        # ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®æˆ–ä½¿ç”¨é»˜è®¤é…ç½®
        config = load_config_from_env()
    
    # ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.model:
        config.model = args.model
    
    if args.devices:
        device_ids = [int(x.strip()) for x in args.devices.split(",")]
        config.device = device_ids
    
    if args.max_tokens:
        config.max_tokens = args.max_tokens
    
    if args.temperature:
        config.temperature = args.temperature
    
    if args.top_p:
        config.top_p = args.top_p
    
    if args.batch_size:
        config.batch_size = args.batch_size
    
    if args.num_prompts:
        config.num_prompts = args.num_prompts
    
    if args.test_iterations:
        config.test_iterations = args.test_iterations
    
    if args.warmup_iterations:
        config.warmup_iterations = args.warmup_iterations
    
    if args.output_dir:
        config.output_dir = args.output_dir
    
    if args.no_save:
        config.save_results = False
    
    return config


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦å®‰è£…"""
    required_packages = {
        "torch": "PyTorch",
        "transformers": "Hugging Face Transformers",
        "vllm": "vLLM",
        "numpy": "NumPy",
        "psutil": "psutil"
    }
    
    missing_packages = []
    
    for package, name in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(name)
    
    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…:")
        for package in missing_packages:
            print(f"   - {package}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–:")
        print("pip install -r requirements.txt")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    return True


def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    print("\n" + "="*60)
    print("ç³»ç»Ÿä¿¡æ¯")
    print("="*60)
    
    try:
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"    å†…å­˜: {memory_total:.1f}GB")
    except ImportError:
        print("PyTorchæœªå®‰è£…")
    
    try:
        import transformers
        print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
    except ImportError:
        print("Transformersæœªå®‰è£…")
    
    try:
        import vllm
        print(f"vLLMç‰ˆæœ¬: {vllm.__version__}")
    except ImportError:
        print("vLLMæœªå®‰è£…")
    
    print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’Œä¼˜åŒ–æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print_system_info()
    
    # åˆ›å»ºé…ç½®
    config = create_config_from_args(args)
    
    print(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
    print(f"  æ¨¡å‹: {config.model}")
    print(f"  è®¾å¤‡: {config.device}")
    print(f"  æœ€å¤§tokens: {config.max_tokens}")
    print(f"  æ¸©åº¦: {config.temperature}")
    print(f"  æµ‹è¯•è¿­ä»£: {config.test_iterations}")
    print(f"  è¾“å‡ºç›®å½•: {config.output_dir}")
    
    # å‡†å¤‡æµ‹è¯•æç¤º
    custom_prompts = None
    if args.prompts:
        custom_prompts = args.prompts
        print(f"  ä½¿ç”¨è‡ªå®šä¹‰æç¤º: {len(custom_prompts)}ä¸ª")
    elif args.prompts_file:
        custom_prompts = load_prompts_from_file(args.prompts_file)
        if not custom_prompts:
            print("âš ï¸ æ— æ³•ä»æ–‡ä»¶åŠ è½½æç¤ºï¼Œä½¿ç”¨é»˜è®¤æç¤º")
    
    try:
        # åˆ›å»ºæ¨ç†æ¨¡å—
        inference_module = InferenceOnPagedAttention(config)
        
        if args.benchmark_memory:
            # å†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•
            batch_sizes = [int(x.strip()) for x in args.batch_sizes.split(",")]
            print(f"\nğŸ§ª å¼€å§‹å†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•...")
            results = inference_module.benchmark_memory_efficiency(batch_sizes)
            print(f"âœ… å†…å­˜åŸºå‡†æµ‹è¯•å®Œæˆ")
            
        else:
            # å¸¸è§„å¯¹æ¯”æµ‹è¯•
            print(f"\nğŸ§ª å¼€å§‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
            if args.method == "both":
                results = inference_module.run_comparison_test(custom_prompts)
            else:
                # ç”Ÿæˆæˆ–ä½¿ç”¨è‡ªå®šä¹‰æç¤º
                if custom_prompts is None:
                    test_prompts = inference_module.generate_test_prompts()
                else:
                    test_prompts = custom_prompts
                
                results = inference_module.inference(test_prompts, method=args.method)
            
            print(f"âœ… æµ‹è¯•å®Œæˆ")
        
        print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {config.output_dir}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        # ç¡®ä¿æ¸…ç†èµ„æº
        try:
            if 'inference_module' in locals():
                inference_module.cleanup()
        except:
            pass


if __name__ == "__main__":
    main()
