"""
ä¸»ç¨‹åº - å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’Œä¼˜åŒ–

è¿™æ˜¯é¡¹ç›®çš„ä¸»å…¥å£æ–‡ä»¶ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å„ä¸ªæ¨¡å—è¿›è¡ŒLLMæ¨ç†æ€§èƒ½å¯¹æ¯”æµ‹è¯•ã€‚

æ³¨æ„ï¼šæœ¬é¡¹ç›®å·²ç»Ÿä¸€é…ç½®ä¸ºä½¿ç”¨ DeepSeek-R1-Distill-Qwen-1.5B æ¨¡å‹ï¼Œ
ç¡®ä¿ PagedAttention ä¸ä¼ ç»Ÿ Key-Value Cache å¯¹æ¯”å®éªŒçš„å…¬å¹³æ€§ã€‚
"""

import argparse
import sys
import os
from typing import List, Optional
import vllm
import torch
import transformers

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.config import Config, load_config_from_yaml, get_config_for_technique, print_available_devices
from inference.inference_paged_attention import InferenceOnPagedAttention
from inference.inference_quantization import InferenceOnQuantization


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="LLMæ¨ç†ä¼˜åŒ–æ€§èƒ½æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # ä¼˜åŒ–æŠ€æœ¯é€‰æ‹©
    parser.add_argument("--technique", type=str, 
                       choices=["PagedAttention", "é‡åŒ–"], 
                       default="PagedAttention",
                       help="é€‰æ‹©ä¼˜åŒ–æŠ€æœ¯")
    
    # æ‰¹æ¬¡çº§åˆ«
    parser.add_argument("--batch-level", type=str,
                       choices=["small", "medium", "large"],
                       default="medium",
                       help="æ‰¹æ¬¡çº§åˆ«")
    
    # å®éªŒåç§°
    parser.add_argument("--experiment-name", type=str, default="default",
                       help="å®éªŒåç§°")
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument("--config", type=str, default=None,
                       help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„")
    
    # è®¾å¤‡é…ç½®
    parser.add_argument("--device", type=str, default=None,
                       help="æŒ‡å®šCUDAè®¾å¤‡ï¼Œä¾‹å¦‚: '0' è¡¨ç¤ºcuda:0, '0,1' è¡¨ç¤ºä½¿ç”¨GPU 0å’Œ1, 'cpu' è¡¨ç¤ºä½¿ç”¨CPU")
    parser.add_argument("--list-devices", action="store_true",
                       help="æ˜¾ç¤ºå¯ç”¨è®¾å¤‡ä¿¡æ¯å¹¶é€€å‡º")
    
    # æ¨ç†é…ç½®
    parser.add_argument("--max-tokens", type=int, default=None,
                       help="æœ€å¤§ç”Ÿæˆtokensæ•°é‡")
    parser.add_argument("--temperature", type=float, default=None,
                       help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=None,
                       help="æ ¸é‡‡æ ·å‚æ•°")
    parser.add_argument("--batch-size", type=int, default=None,
                       help="æ‰¹æ¬¡å¤§å°")
    
    # æµ‹è¯•é…ç½®
    parser.add_argument("--num-prompts", type=int, default=None,
                       help="æµ‹è¯•æç¤ºæ•°é‡")
    parser.add_argument("--test-iterations", type=int, default=None,
                       help="æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--warmup-iterations", type=int, default=None,
                       help="é¢„çƒ­è¿­ä»£æ¬¡æ•°")
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--method", type=str, 
                       choices=["basic", "optimized", "both"], 
                       default="both",
                       help="æ¨ç†æ–¹æ³•")
    
    # åŸºå‡†æµ‹è¯•
    parser.add_argument("--benchmark-memory", action="store_true",
                       help="è¿è¡Œå†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•")
    parser.add_argument("--batch-sizes", type=str, default="1,2,4,8",
                       help="å†…å­˜åŸºå‡†æµ‹è¯•çš„æ‰¹æ¬¡å¤§å°ï¼Œç”¨é€—å·åˆ†éš”")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--output-dir", type=str, default=None,
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--no-save", action="store_true",
                       help="ä¸ä¿å­˜ç»“æœæ–‡ä»¶")
    
    # è‡ªå®šä¹‰æç¤º
    parser.add_argument("--prompts", nargs="+", default=None,
                       help="è‡ªå®šä¹‰æµ‹è¯•æç¤º")
    parser.add_argument("--prompts-file", type=str, default=None,
                       help="ä»æ–‡ä»¶è¯»å–æµ‹è¯•æç¤º")
    
    return parser.parse_args()


def load_prompts_from_file(filepath: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æç¤º"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f if line.strip()]
        print(f"ä»æ–‡ä»¶ {filepath} åŠ è½½äº† {len(prompts)} ä¸ªæç¤º")
        return prompts
    except Exception as e:
        print(f"æ— æ³•ä»æ–‡ä»¶åŠ è½½æç¤º: {e}")
        return []


def create_config_from_args(args) -> Config:
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®"""
    
    # ä»æŒ‡å®šé…ç½®æ–‡ä»¶åŠ è½½
    if args.config:
        config = load_config_from_yaml(args.config)
    else:
        # æ ¹æ®æŠ€æœ¯å’Œæ‰¹æ¬¡çº§åˆ«è·å–é…ç½®
        config = get_config_for_technique(
            technique=args.technique,
            batch_level=args.batch_level,
            experiment_name=args.experiment_name
        )
    
    # ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.device:
        # è§£æè®¾å¤‡å‚æ•°
        if args.device.lower() == 'cpu':
            config.device = []
        else:
            try:
                # æ”¯æŒ "0" æˆ– "0,1,2" æ ¼å¼
                device_ids = [int(x.strip()) for x in args.device.split(',')]
                config.device = device_ids
                print(f"è®¾ç½®è®¾å¤‡ä¸º: {device_ids}")
            except ValueError:
                print(f"è­¦å‘Š: æ— æ•ˆçš„è®¾å¤‡å‚æ•° '{args.device}'ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡")
    
    if args.max_tokens:
        config.max_tokens = args.max_tokens
    
    if args.temperature:
        config.temperature = args.temperature
    
    if args.top_p:
        config.top_p = args.top_p
    
    if args.batch_size:
        config.batch_size = args.batch_size
    
    if args.num_prompts:
        config.num_prompts = args.num_prompts
    
    if args.test_iterations:
        config.test_iterations = args.test_iterations
    
    if args.warmup_iterations:
        config.warmup_iterations = args.warmup_iterations
    
    if args.output_dir:
        config.output_dir = args.output_dir
    
    if args.no_save:
        config.save_results = False
    
    return config



def main():

    # # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¡®ä¿CUDAåº“çš„æ­£ç¡®åŠ è½½
    # os.environ["LD_LIBRARY_PATH"] = "/home/zhaofanghan/tmp/lib:/home/zhaofanghan/tmp/cuda_stubs:" + os.environ.get("LD_LIBRARY_PATH", "")
    # os.environ["CUDA_HOME"] = "/usr/local/cuda"
    # os.environ["PATH"] = "/usr/local/cuda/bin:" + os.environ.get("PATH", "")
    
    
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’Œä¼˜åŒ–æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # å¦‚æœç”¨æˆ·è¯·æ±‚æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯ï¼Œæ˜¾ç¤ºåé€€å‡º
    if args.list_devices:
        print_available_devices()
        return
    
    # åˆ›å»ºé…ç½®
    config = create_config_from_args(args)
    
    print(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
    print(f"  ä¼˜åŒ–æŠ€æœ¯: {config.technique}")
    print(f"  æ‰¹æ¬¡çº§åˆ«: {config.batch_level}")
    print(f"  å®éªŒåç§°: {config.experiment_name}")
    print(f"  æ¨¡å‹: {config.model}")
    print(f"  è®¾å¤‡: {config.device}")
    print(f"  æœ€å¤§tokens: {config.max_tokens}")
    print(f"  æ¸©åº¦: {config.temperature}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  æµ‹è¯•è¿­ä»£: {config.test_iterations}")
    print(f"  è¾“å‡ºç›®å½•: {config.output_dir}")
    
    # å‡†å¤‡æµ‹è¯•æç¤º
    custom_prompts = None
    if args.prompts:
        custom_prompts = args.prompts
        print(f"  ä½¿ç”¨è‡ªå®šä¹‰æç¤º: {len(custom_prompts)}ä¸ª")
    elif args.prompts_file:
        custom_prompts = load_prompts_from_file(args.prompts_file)
        if not custom_prompts:
            print("âš ï¸ æ— æ³•ä»æ–‡ä»¶åŠ è½½æç¤ºï¼Œä½¿ç”¨é»˜è®¤æç¤º")
    
    
    
    try:
        # æ ¹æ®ä¼˜åŒ–æŠ€æœ¯åˆ›å»ºæ¨ç†æ¨¡å—
        if config.technique == "PagedAttention":
            inference_module = InferenceOnPagedAttention(config)
        elif config.technique == "é‡åŒ–":
            inference_module = InferenceOnQuantization(config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–æŠ€æœ¯: {config.technique}")
        
        if args.benchmark_memory:
            # å†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•
            if hasattr(inference_module, 'benchmark_memory_efficiency'):
                batch_sizes = [int(x.strip()) for x in args.batch_sizes.split(",")]
                print(f"\nğŸ§ª å¼€å§‹å†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•...")
                results = inference_module.benchmark_memory_efficiency(batch_sizes)
                print(f"âœ… å†…å­˜åŸºå‡†æµ‹è¯•å®Œæˆ")
            else:
                print(f"âš ï¸ {config.technique} ä¸æ”¯æŒå†…å­˜åŸºå‡†æµ‹è¯•")
                return
            
        else:
            # å¸¸è§„å¯¹æ¯”æµ‹è¯•
            print(f"\nğŸ§ª å¼€å§‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
            if args.method == "both":
                results = inference_module.run_comparison_test(custom_prompts)
            else:
                # ç”Ÿæˆæˆ–ä½¿ç”¨è‡ªå®šä¹‰æç¤º
                if custom_prompts is None:
                    test_prompts = inference_module.generate_test_prompts()
                else:
                    test_prompts = custom_prompts
                
                results = inference_module.inference(test_prompts, method=args.method)
            
            print(f"âœ… æµ‹è¯•å®Œæˆ")
        
        print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {config.output_dir}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        # ç¡®ä¿æ¸…ç†èµ„æº
        try:
            if 'inference_module' in locals():
                inference_module.cleanup()
        except:
            pass


if __name__ == "__main__":
    main()
